//
// Generated by NVIDIA NVVM Compiler
// Compiler built on Thu Mar 13 14:31:35 2014 (1394735495)
// Cuda compilation tools, release 6.0, V6.0.1
//

.version 4.0
.target sm_35
.address_size 64

.global .align 4 .b8 gm_idx_pool[8388480];
.shared .align 8 .b8 _Z16block_reduce_sumd$__cuda_local_var_35746_40_non_const_shared[256];

.weak .func  (.param .b32 func_retval0) cudaMalloc(
	.param .b64 cudaMalloc_param_0,
	.param .b64 cudaMalloc_param_1
)
{
	.reg .s32 	%r<2>;


	mov.u32 	%r1, 30;
	st.param.b32	[func_retval0+0], %r1;
	ret;
}

.weak .func  (.param .b32 func_retval0) cudaFuncGetAttributes(
	.param .b64 cudaFuncGetAttributes_param_0,
	.param .b64 cudaFuncGetAttributes_param_1
)
{
	.reg .s32 	%r<2>;


	mov.u32 	%r1, 30;
	st.param.b32	[func_retval0+0], %r1;
	ret;
}

.visible .func  (.param .b64 func_retval0) _Z15warp_reduce_sumd(
	.param .b64 _Z15warp_reduce_sumd_param_0
)
{
	.reg .s32 	%r<41>;
	.reg .s64 	%rd<31>;
	.reg .f64 	%fd<12>;


	ld.param.f64 	%fd1, [_Z15warp_reduce_sumd_param_0];
	mov.b64 	 %rd1, %fd1;
	cvt.u32.u64	%r2, %rd1;
	mov.u32 	%r7, 16;
	mov.u32 	%r40, 31;
	// inline asm
	shfl.down.b32 %r1, %r2, %r7, %r40;
	// inline asm
	cvt.u64.u32	%rd2, %r1;
	shr.u64 	%rd3, %rd1, 32;
	cvt.u32.u64	%r6, %rd3;
	// inline asm
	shfl.down.b32 %r5, %r6, %r7, %r40;
	// inline asm
	cvt.u64.u32	%rd4, %r5;
	shl.b64 	%rd5, %rd4, 32;
	or.b64  	%rd6, %rd5, %rd2;
	mov.b64 	 %fd2, %rd6;
	add.f64 	%fd3, %fd2, %fd1;
	mov.b64 	 %rd7, %fd3;
	cvt.u32.u64	%r10, %rd7;
	mov.u32 	%r15, 8;
	// inline asm
	shfl.down.b32 %r9, %r10, %r15, %r40;
	// inline asm
	cvt.u64.u32	%rd8, %r9;
	shr.u64 	%rd9, %rd7, 32;
	cvt.u32.u64	%r14, %rd9;
	// inline asm
	shfl.down.b32 %r13, %r14, %r15, %r40;
	// inline asm
	cvt.u64.u32	%rd10, %r13;
	shl.b64 	%rd11, %rd10, 32;
	or.b64  	%rd12, %rd11, %rd8;
	mov.b64 	 %fd4, %rd12;
	add.f64 	%fd5, %fd3, %fd4;
	mov.b64 	 %rd13, %fd5;
	cvt.u32.u64	%r18, %rd13;
	mov.u32 	%r23, 4;
	// inline asm
	shfl.down.b32 %r17, %r18, %r23, %r40;
	// inline asm
	cvt.u64.u32	%rd14, %r17;
	shr.u64 	%rd15, %rd13, 32;
	cvt.u32.u64	%r22, %rd15;
	// inline asm
	shfl.down.b32 %r21, %r22, %r23, %r40;
	// inline asm
	cvt.u64.u32	%rd16, %r21;
	shl.b64 	%rd17, %rd16, 32;
	or.b64  	%rd18, %rd17, %rd14;
	mov.b64 	 %fd6, %rd18;
	add.f64 	%fd7, %fd5, %fd6;
	mov.b64 	 %rd19, %fd7;
	cvt.u32.u64	%r26, %rd19;
	mov.u32 	%r31, 2;
	// inline asm
	shfl.down.b32 %r25, %r26, %r31, %r40;
	// inline asm
	cvt.u64.u32	%rd20, %r25;
	shr.u64 	%rd21, %rd19, 32;
	cvt.u32.u64	%r30, %rd21;
	// inline asm
	shfl.down.b32 %r29, %r30, %r31, %r40;
	// inline asm
	cvt.u64.u32	%rd22, %r29;
	shl.b64 	%rd23, %rd22, 32;
	or.b64  	%rd24, %rd23, %rd20;
	mov.b64 	 %fd8, %rd24;
	add.f64 	%fd9, %fd7, %fd8;
	mov.b64 	 %rd25, %fd9;
	cvt.u32.u64	%r34, %rd25;
	mov.u32 	%r39, 1;
	// inline asm
	shfl.down.b32 %r33, %r34, %r39, %r40;
	// inline asm
	cvt.u64.u32	%rd26, %r33;
	shr.u64 	%rd27, %rd25, 32;
	cvt.u32.u64	%r38, %rd27;
	// inline asm
	shfl.down.b32 %r37, %r38, %r39, %r40;
	// inline asm
	cvt.u64.u32	%rd28, %r37;
	shl.b64 	%rd29, %rd28, 32;
	or.b64  	%rd30, %rd29, %rd26;
	mov.b64 	 %fd10, %rd30;
	add.f64 	%fd11, %fd9, %fd10;
	st.param.f64	[func_retval0+0], %fd11;
	ret;
}

.visible .func  (.param .b64 func_retval0) _Z16block_reduce_sumd(
	.param .b64 _Z16block_reduce_sumd_param_0
)
{
	.reg .pred 	%p<4>;
	.reg .s32 	%r<86>;
	.reg .s64 	%rd<67>;
	.reg .f64 	%fd<27>;


	ld.param.f64 	%fd6, [_Z16block_reduce_sumd_param_0];
	mov.u32 	%r1, %tid.x;
	and.b32  	%r2, %r1, 31;
	mov.u32 	%r43, 31;
	mov.b64 	 %rd1, %fd6;
	cvt.u32.u64	%r5, %rd1;
	mov.u32 	%r10, 16;
	// inline asm
	shfl.down.b32 %r4, %r5, %r10, %r43;
	// inline asm
	cvt.u64.u32	%rd2, %r4;
	shr.u64 	%rd3, %rd1, 32;
	cvt.u32.u64	%r9, %rd3;
	// inline asm
	shfl.down.b32 %r8, %r9, %r10, %r43;
	// inline asm
	cvt.u64.u32	%rd4, %r8;
	shl.b64 	%rd5, %rd4, 32;
	or.b64  	%rd6, %rd5, %rd2;
	mov.b64 	 %fd7, %rd6;
	add.f64 	%fd8, %fd7, %fd6;
	mov.b64 	 %rd7, %fd8;
	cvt.u32.u64	%r13, %rd7;
	mov.u32 	%r18, 8;
	// inline asm
	shfl.down.b32 %r12, %r13, %r18, %r43;
	// inline asm
	cvt.u64.u32	%rd8, %r12;
	shr.u64 	%rd9, %rd7, 32;
	cvt.u32.u64	%r17, %rd9;
	// inline asm
	shfl.down.b32 %r16, %r17, %r18, %r43;
	// inline asm
	cvt.u64.u32	%rd10, %r16;
	shl.b64 	%rd11, %rd10, 32;
	or.b64  	%rd12, %rd11, %rd8;
	mov.b64 	 %fd9, %rd12;
	add.f64 	%fd10, %fd8, %fd9;
	mov.b64 	 %rd13, %fd10;
	cvt.u32.u64	%r21, %rd13;
	mov.u32 	%r26, 4;
	// inline asm
	shfl.down.b32 %r20, %r21, %r26, %r43;
	// inline asm
	cvt.u64.u32	%rd14, %r20;
	shr.u64 	%rd15, %rd13, 32;
	cvt.u32.u64	%r25, %rd15;
	// inline asm
	shfl.down.b32 %r24, %r25, %r26, %r43;
	// inline asm
	cvt.u64.u32	%rd16, %r24;
	shl.b64 	%rd17, %rd16, 32;
	or.b64  	%rd18, %rd17, %rd14;
	mov.b64 	 %fd11, %rd18;
	add.f64 	%fd12, %fd10, %fd11;
	mov.b64 	 %rd19, %fd12;
	cvt.u32.u64	%r29, %rd19;
	mov.u32 	%r34, 2;
	// inline asm
	shfl.down.b32 %r28, %r29, %r34, %r43;
	// inline asm
	cvt.u64.u32	%rd20, %r28;
	shr.u64 	%rd21, %rd19, 32;
	cvt.u32.u64	%r33, %rd21;
	// inline asm
	shfl.down.b32 %r32, %r33, %r34, %r43;
	// inline asm
	cvt.u64.u32	%rd22, %r32;
	shl.b64 	%rd23, %rd22, 32;
	or.b64  	%rd24, %rd23, %rd20;
	mov.b64 	 %fd13, %rd24;
	add.f64 	%fd14, %fd12, %fd13;
	mov.b64 	 %rd25, %fd14;
	cvt.u32.u64	%r37, %rd25;
	mov.u32 	%r42, 1;
	// inline asm
	shfl.down.b32 %r36, %r37, %r42, %r43;
	// inline asm
	cvt.u64.u32	%rd26, %r36;
	shr.u64 	%rd27, %rd25, 32;
	cvt.u32.u64	%r41, %rd27;
	// inline asm
	shfl.down.b32 %r40, %r41, %r42, %r43;
	// inline asm
	cvt.u64.u32	%rd28, %r40;
	shl.b64 	%rd29, %rd28, 32;
	or.b64  	%rd30, %rd29, %rd26;
	mov.b64 	 %fd15, %rd30;
	add.f64 	%fd1, %fd14, %fd15;
	shr.u32 	%r3, %r1, 5;
	setp.ne.s32	%p1, %r2, 0;
	@%p1 bra 	BB3_2;

	mul.wide.s32 	%rd31, %r3, 8;
	mov.u64 	%rd32, _Z16block_reduce_sumd$__cuda_local_var_35746_40_non_const_shared;
	add.s64 	%rd33, %rd32, %rd31;
	st.shared.f64 	[%rd33], %fd1;

BB3_2:
	bar.sync 	0;
	mov.u32 	%r44, %ntid.x;
	shr.u32 	%r45, %r44, 5;
	setp.lt.u32	%p2, %r1, %r45;
	@%p2 bra 	BB3_4;

	mov.f64 	%fd26, 0d0000000000000000;
	bra.uni 	BB3_5;

BB3_4:
	mul.wide.s32 	%rd34, %r2, 8;
	mov.u64 	%rd35, _Z16block_reduce_sumd$__cuda_local_var_35746_40_non_const_shared;
	add.s64 	%rd36, %rd35, %rd34;
	ld.shared.f64 	%fd26, [%rd36];

BB3_5:
	setp.ne.s32	%p3, %r3, 0;
	@%p3 bra 	BB3_7;

	mov.b64 	 %rd37, %fd26;
	cvt.u32.u64	%r47, %rd37;
	// inline asm
	shfl.down.b32 %r46, %r47, %r10, %r43;
	// inline asm
	cvt.u64.u32	%rd38, %r46;
	shr.u64 	%rd39, %rd37, 32;
	cvt.u32.u64	%r51, %rd39;
	// inline asm
	shfl.down.b32 %r50, %r51, %r10, %r43;
	// inline asm
	cvt.u64.u32	%rd40, %r50;
	shl.b64 	%rd41, %rd40, 32;
	or.b64  	%rd42, %rd41, %rd38;
	mov.b64 	 %fd17, %rd42;
	add.f64 	%fd18, %fd26, %fd17;
	mov.b64 	 %rd43, %fd18;
	cvt.u32.u64	%r55, %rd43;
	// inline asm
	shfl.down.b32 %r54, %r55, %r18, %r43;
	// inline asm
	cvt.u64.u32	%rd44, %r54;
	shr.u64 	%rd45, %rd43, 32;
	cvt.u32.u64	%r59, %rd45;
	// inline asm
	shfl.down.b32 %r58, %r59, %r18, %r43;
	// inline asm
	cvt.u64.u32	%rd46, %r58;
	shl.b64 	%rd47, %rd46, 32;
	or.b64  	%rd48, %rd47, %rd44;
	mov.b64 	 %fd19, %rd48;
	add.f64 	%fd20, %fd18, %fd19;
	mov.b64 	 %rd49, %fd20;
	cvt.u32.u64	%r63, %rd49;
	// inline asm
	shfl.down.b32 %r62, %r63, %r26, %r43;
	// inline asm
	cvt.u64.u32	%rd50, %r62;
	shr.u64 	%rd51, %rd49, 32;
	cvt.u32.u64	%r67, %rd51;
	// inline asm
	shfl.down.b32 %r66, %r67, %r26, %r43;
	// inline asm
	cvt.u64.u32	%rd52, %r66;
	shl.b64 	%rd53, %rd52, 32;
	or.b64  	%rd54, %rd53, %rd50;
	mov.b64 	 %fd21, %rd54;
	add.f64 	%fd22, %fd20, %fd21;
	mov.b64 	 %rd55, %fd22;
	cvt.u32.u64	%r71, %rd55;
	// inline asm
	shfl.down.b32 %r70, %r71, %r34, %r43;
	// inline asm
	cvt.u64.u32	%rd56, %r70;
	shr.u64 	%rd57, %rd55, 32;
	cvt.u32.u64	%r75, %rd57;
	// inline asm
	shfl.down.b32 %r74, %r75, %r34, %r43;
	// inline asm
	cvt.u64.u32	%rd58, %r74;
	shl.b64 	%rd59, %rd58, 32;
	or.b64  	%rd60, %rd59, %rd56;
	mov.b64 	 %fd23, %rd60;
	add.f64 	%fd24, %fd22, %fd23;
	mov.b64 	 %rd61, %fd24;
	cvt.u32.u64	%r79, %rd61;
	// inline asm
	shfl.down.b32 %r78, %r79, %r42, %r43;
	// inline asm
	cvt.u64.u32	%rd62, %r78;
	shr.u64 	%rd63, %rd61, 32;
	cvt.u32.u64	%r83, %rd63;
	// inline asm
	shfl.down.b32 %r82, %r83, %r42, %r43;
	// inline asm
	cvt.u64.u32	%rd64, %r82;
	shl.b64 	%rd65, %rd64, 32;
	or.b64  	%rd66, %rd65, %rd62;
	mov.b64 	 %fd25, %rd66;
	add.f64 	%fd26, %fd24, %fd25;

BB3_7:
	st.param.f64	[func_retval0+0], %fd26;
	ret;
}

.visible .func  (.param .b64 func_retval0) _Z9atomicAddPdd(
	.param .b64 _Z9atomicAddPdd_param_0,
	.param .b64 _Z9atomicAddPdd_param_1
)
{
	.reg .pred 	%p<2>;
	.reg .s64 	%rd<7>;
	.reg .f64 	%fd<5>;


	ld.param.u64 	%rd4, [_Z9atomicAddPdd_param_0];
	ld.param.f64 	%fd1, [_Z9atomicAddPdd_param_1];
	ld.u64 	%rd6, [%rd4];

BB4_1:
	mov.u64 	%rd2, %rd6;
	mov.b64 	 %fd2, %rd2;
	add.f64 	%fd3, %fd2, %fd1;
	mov.b64 	 %rd5, %fd3;
	atom.cas.b64 	%rd6, [%rd4], %rd2, %rd5;
	setp.ne.s64	%p1, %rd2, %rd6;
	@%p1 bra 	BB4_1;

	mov.b64 	 %fd4, %rd6;
	st.param.f64	[func_retval0+0], %fd4;
	ret;
}

.visible .entry _Z20pg_process_neighborsPiS_PfS0_fffiii(
	.param .u64 _Z20pg_process_neighborsPiS_PfS0_fffiii_param_0,
	.param .u64 _Z20pg_process_neighborsPiS_PfS0_fffiii_param_1,
	.param .u64 _Z20pg_process_neighborsPiS_PfS0_fffiii_param_2,
	.param .u64 _Z20pg_process_neighborsPiS_PfS0_fffiii_param_3,
	.param .f32 _Z20pg_process_neighborsPiS_PfS0_fffiii_param_4,
	.param .f32 _Z20pg_process_neighborsPiS_PfS0_fffiii_param_5,
	.param .f32 _Z20pg_process_neighborsPiS_PfS0_fffiii_param_6,
	.param .u32 _Z20pg_process_neighborsPiS_PfS0_fffiii_param_7,
	.param .u32 _Z20pg_process_neighborsPiS_PfS0_fffiii_param_8,
	.param .u32 _Z20pg_process_neighborsPiS_PfS0_fffiii_param_9
)
{
	.reg .pred 	%p<7>;
	.reg .s32 	%r<94>;
	.reg .f32 	%f<12>;
	.reg .s64 	%rd<85>;
	.reg .f64 	%fd<30>;


	ld.param.u64 	%rd2, [_Z20pg_process_neighborsPiS_PfS0_fffiii_param_0];
	ld.param.u64 	%rd3, [_Z20pg_process_neighborsPiS_PfS0_fffiii_param_1];
	ld.param.u64 	%rd4, [_Z20pg_process_neighborsPiS_PfS0_fffiii_param_2];
	ld.param.u64 	%rd5, [_Z20pg_process_neighborsPiS_PfS0_fffiii_param_3];
	ld.param.f32 	%f1, [_Z20pg_process_neighborsPiS_PfS0_fffiii_param_4];
	ld.param.f32 	%f2, [_Z20pg_process_neighborsPiS_PfS0_fffiii_param_5];
	ld.param.f32 	%f3, [_Z20pg_process_neighborsPiS_PfS0_fffiii_param_6];
	ld.param.u32 	%r8, [_Z20pg_process_neighborsPiS_PfS0_fffiii_param_7];
	ld.param.u32 	%r9, [_Z20pg_process_neighborsPiS_PfS0_fffiii_param_8];
	ld.param.u32 	%r7, [_Z20pg_process_neighborsPiS_PfS0_fffiii_param_9];
	cvta.to.global.u64 	%rd1, %rd5;
	mov.u32 	%r1, %ntid.x;
	mov.u32 	%r2, %ctaid.x;
	mov.u32 	%r3, %tid.x;
	add.s32 	%r10, %r3, %r8;
	mad.lo.s32 	%r4, %r1, %r2, %r10;
	setp.lt.s32	%p1, %r4, %r9;
	@%p1 bra 	BB5_2;

	mov.f64 	%fd28, 0d0000000000000000;
	bra.uni 	BB5_3;

BB5_2:
	cvta.to.global.u64 	%rd6, %rd3;
	cvta.to.global.u64 	%rd7, %rd4;
	cvta.to.global.u64 	%rd8, %rd2;
	mul.wide.s32 	%rd9, %r4, 4;
	add.s64 	%rd10, %rd8, %rd9;
	ld.global.s32 	%rd11, [%rd10];
	shl.b64 	%rd12, %rd11, 2;
	add.s64 	%rd13, %rd7, %rd12;
	add.s64 	%rd14, %rd6, %rd12;
	ld.global.u32 	%r11, [%rd14];
	cvt.rn.f32.s32	%f4, %r11;
	ld.global.f32 	%f5, [%rd13];
	div.rn.f32 	%f6, %f5, %f4;
	fma.rn.f32 	%f7, %f6, %f3, 0f00000000;
	cvt.f64.f32	%fd28, %f7;

BB5_3:
	bar.sync 	0;
	mov.b64 	 %rd15, %fd28;
	cvt.u32.u64	%r13, %rd15;
	mov.u32 	%r18, 16;
	mov.u32 	%r51, 31;
	// inline asm
	shfl.down.b32 %r12, %r13, %r18, %r51;
	// inline asm
	cvt.u64.u32	%rd16, %r12;
	shr.u64 	%rd17, %rd15, 32;
	cvt.u32.u64	%r17, %rd17;
	// inline asm
	shfl.down.b32 %r16, %r17, %r18, %r51;
	// inline asm
	cvt.u64.u32	%rd18, %r16;
	shl.b64 	%rd19, %rd18, 32;
	or.b64  	%rd20, %rd19, %rd16;
	mov.b64 	 %fd9, %rd20;
	add.f64 	%fd10, %fd28, %fd9;
	mov.b64 	 %rd21, %fd10;
	cvt.u32.u64	%r21, %rd21;
	mov.u32 	%r26, 8;
	// inline asm
	shfl.down.b32 %r20, %r21, %r26, %r51;
	// inline asm
	cvt.u64.u32	%rd22, %r20;
	shr.u64 	%rd23, %rd21, 32;
	cvt.u32.u64	%r25, %rd23;
	// inline asm
	shfl.down.b32 %r24, %r25, %r26, %r51;
	// inline asm
	cvt.u64.u32	%rd24, %r24;
	shl.b64 	%rd25, %rd24, 32;
	or.b64  	%rd26, %rd25, %rd22;
	mov.b64 	 %fd11, %rd26;
	add.f64 	%fd12, %fd10, %fd11;
	mov.b64 	 %rd27, %fd12;
	cvt.u32.u64	%r29, %rd27;
	mov.u32 	%r34, 4;
	// inline asm
	shfl.down.b32 %r28, %r29, %r34, %r51;
	// inline asm
	cvt.u64.u32	%rd28, %r28;
	shr.u64 	%rd29, %rd27, 32;
	cvt.u32.u64	%r33, %rd29;
	// inline asm
	shfl.down.b32 %r32, %r33, %r34, %r51;
	// inline asm
	cvt.u64.u32	%rd30, %r32;
	shl.b64 	%rd31, %rd30, 32;
	or.b64  	%rd32, %rd31, %rd28;
	mov.b64 	 %fd13, %rd32;
	add.f64 	%fd14, %fd12, %fd13;
	mov.b64 	 %rd33, %fd14;
	cvt.u32.u64	%r37, %rd33;
	mov.u32 	%r42, 2;
	// inline asm
	shfl.down.b32 %r36, %r37, %r42, %r51;
	// inline asm
	cvt.u64.u32	%rd34, %r36;
	shr.u64 	%rd35, %rd33, 32;
	cvt.u32.u64	%r41, %rd35;
	// inline asm
	shfl.down.b32 %r40, %r41, %r42, %r51;
	// inline asm
	cvt.u64.u32	%rd36, %r40;
	shl.b64 	%rd37, %rd36, 32;
	or.b64  	%rd38, %rd37, %rd34;
	mov.b64 	 %fd15, %rd38;
	add.f64 	%fd16, %fd14, %fd15;
	mov.b64 	 %rd39, %fd16;
	cvt.u32.u64	%r45, %rd39;
	mov.u32 	%r50, 1;
	// inline asm
	shfl.down.b32 %r44, %r45, %r50, %r51;
	// inline asm
	cvt.u64.u32	%rd40, %r44;
	shr.u64 	%rd41, %rd39, 32;
	cvt.u32.u64	%r49, %rd41;
	// inline asm
	shfl.down.b32 %r48, %r49, %r50, %r51;
	// inline asm
	cvt.u64.u32	%rd42, %r48;
	shl.b64 	%rd43, %rd42, 32;
	or.b64  	%rd44, %rd43, %rd40;
	mov.b64 	 %fd17, %rd44;
	add.f64 	%fd3, %fd16, %fd17;
	shr.u32 	%r5, %r3, 5;
	and.b32  	%r6, %r3, 31;
	setp.ne.s32	%p2, %r6, 0;
	@%p2 bra 	BB5_5;

	mul.wide.u32 	%rd45, %r5, 8;
	mov.u64 	%rd46, _Z16block_reduce_sumd$__cuda_local_var_35746_40_non_const_shared;
	add.s64 	%rd47, %rd46, %rd45;
	st.shared.f64 	[%rd47], %fd3;

BB5_5:
	bar.sync 	0;
	shr.u32 	%r52, %r1, 5;
	setp.lt.u32	%p3, %r3, %r52;
	@%p3 bra 	BB5_7;

	mov.f64 	%fd29, 0d0000000000000000;
	bra.uni 	BB5_8;

BB5_7:
	mul.wide.u32 	%rd48, %r6, 8;
	mov.u64 	%rd49, _Z16block_reduce_sumd$__cuda_local_var_35746_40_non_const_shared;
	add.s64 	%rd50, %rd49, %rd48;
	ld.shared.f64 	%fd29, [%rd50];

BB5_8:
	setp.ne.s32	%p4, %r5, 0;
	@%p4 bra 	BB5_10;

	mov.b64 	 %rd51, %fd29;
	cvt.u32.u64	%r54, %rd51;
	// inline asm
	shfl.down.b32 %r53, %r54, %r18, %r51;
	// inline asm
	cvt.u64.u32	%rd52, %r53;
	shr.u64 	%rd53, %rd51, 32;
	cvt.u32.u64	%r58, %rd53;
	// inline asm
	shfl.down.b32 %r57, %r58, %r18, %r51;
	// inline asm
	cvt.u64.u32	%rd54, %r57;
	shl.b64 	%rd55, %rd54, 32;
	or.b64  	%rd56, %rd55, %rd52;
	mov.b64 	 %fd19, %rd56;
	add.f64 	%fd20, %fd29, %fd19;
	mov.b64 	 %rd57, %fd20;
	cvt.u32.u64	%r62, %rd57;
	// inline asm
	shfl.down.b32 %r61, %r62, %r26, %r51;
	// inline asm
	cvt.u64.u32	%rd58, %r61;
	shr.u64 	%rd59, %rd57, 32;
	cvt.u32.u64	%r66, %rd59;
	// inline asm
	shfl.down.b32 %r65, %r66, %r26, %r51;
	// inline asm
	cvt.u64.u32	%rd60, %r65;
	shl.b64 	%rd61, %rd60, 32;
	or.b64  	%rd62, %rd61, %rd58;
	mov.b64 	 %fd21, %rd62;
	add.f64 	%fd22, %fd20, %fd21;
	mov.b64 	 %rd63, %fd22;
	cvt.u32.u64	%r70, %rd63;
	// inline asm
	shfl.down.b32 %r69, %r70, %r34, %r51;
	// inline asm
	cvt.u64.u32	%rd64, %r69;
	shr.u64 	%rd65, %rd63, 32;
	cvt.u32.u64	%r74, %rd65;
	// inline asm
	shfl.down.b32 %r73, %r74, %r34, %r51;
	// inline asm
	cvt.u64.u32	%rd66, %r73;
	shl.b64 	%rd67, %rd66, 32;
	or.b64  	%rd68, %rd67, %rd64;
	mov.b64 	 %fd23, %rd68;
	add.f64 	%fd24, %fd22, %fd23;
	mov.b64 	 %rd69, %fd24;
	cvt.u32.u64	%r78, %rd69;
	// inline asm
	shfl.down.b32 %r77, %r78, %r42, %r51;
	// inline asm
	cvt.u64.u32	%rd70, %r77;
	shr.u64 	%rd71, %rd69, 32;
	cvt.u32.u64	%r82, %rd71;
	// inline asm
	shfl.down.b32 %r81, %r82, %r42, %r51;
	// inline asm
	cvt.u64.u32	%rd72, %r81;
	shl.b64 	%rd73, %rd72, 32;
	or.b64  	%rd74, %rd73, %rd70;
	mov.b64 	 %fd25, %rd74;
	add.f64 	%fd26, %fd24, %fd25;
	mov.b64 	 %rd75, %fd26;
	cvt.u32.u64	%r86, %rd75;
	// inline asm
	shfl.down.b32 %r85, %r86, %r50, %r51;
	// inline asm
	cvt.u64.u32	%rd76, %r85;
	shr.u64 	%rd77, %rd75, 32;
	cvt.u32.u64	%r90, %rd77;
	// inline asm
	shfl.down.b32 %r89, %r90, %r50, %r51;
	// inline asm
	cvt.u64.u32	%rd78, %r89;
	shl.b64 	%rd79, %rd78, 32;
	or.b64  	%rd80, %rd79, %rd76;
	mov.b64 	 %fd27, %rd80;
	add.f64 	%fd29, %fd26, %fd27;

BB5_10:
	setp.ne.s32	%p5, %r3, 0;
	@%p5 bra 	BB5_12;

	cvt.rn.f32.f64	%f8, %fd29;
	mul.wide.s32 	%rd81, %r7, 4;
	add.s64 	%rd82, %rd1, %rd81;
	atom.global.add.f32 	%f9, [%rd82], %f8;

BB5_12:
	or.b32  	%r93, %r3, %r2;
	setp.ne.s32	%p6, %r93, 0;
	@%p6 bra 	BB5_14;

	mul.wide.s32 	%rd83, %r7, 4;
	add.s64 	%rd84, %rd1, %rd83;
	add.f32 	%f10, %f1, %f2;
	atom.global.add.f32 	%f11, [%rd84], %f10;

BB5_14:
	ret;
}

.visible .entry _Z17pg_process_bufferPiS_S_PfS0_fffiS_j(
	.param .u64 _Z17pg_process_bufferPiS_S_PfS0_fffiS_j_param_0,
	.param .u64 _Z17pg_process_bufferPiS_S_PfS0_fffiS_j_param_1,
	.param .u64 _Z17pg_process_bufferPiS_S_PfS0_fffiS_j_param_2,
	.param .u64 _Z17pg_process_bufferPiS_S_PfS0_fffiS_j_param_3,
	.param .u64 _Z17pg_process_bufferPiS_S_PfS0_fffiS_j_param_4,
	.param .f32 _Z17pg_process_bufferPiS_S_PfS0_fffiS_j_param_5,
	.param .f32 _Z17pg_process_bufferPiS_S_PfS0_fffiS_j_param_6,
	.param .f32 _Z17pg_process_bufferPiS_S_PfS0_fffiS_j_param_7,
	.param .u32 _Z17pg_process_bufferPiS_S_PfS0_fffiS_j_param_8,
	.param .u64 _Z17pg_process_bufferPiS_S_PfS0_fffiS_j_param_9,
	.param .u32 _Z17pg_process_bufferPiS_S_PfS0_fffiS_j_param_10
)
{
	.reg .pred 	%p<8>;
	.reg .s32 	%r<96>;
	.reg .f32 	%f<14>;
	.reg .s64 	%rd<92>;
	.reg .f64 	%fd<30>;


	ld.param.u64 	%rd6, [_Z17pg_process_bufferPiS_S_PfS0_fffiS_j_param_0];
	ld.param.u64 	%rd7, [_Z17pg_process_bufferPiS_S_PfS0_fffiS_j_param_1];
	ld.param.u64 	%rd8, [_Z17pg_process_bufferPiS_S_PfS0_fffiS_j_param_2];
	ld.param.u64 	%rd9, [_Z17pg_process_bufferPiS_S_PfS0_fffiS_j_param_3];
	ld.param.u64 	%rd10, [_Z17pg_process_bufferPiS_S_PfS0_fffiS_j_param_4];
	ld.param.f32 	%f3, [_Z17pg_process_bufferPiS_S_PfS0_fffiS_j_param_5];
	ld.param.f32 	%f4, [_Z17pg_process_bufferPiS_S_PfS0_fffiS_j_param_6];
	ld.param.f32 	%f5, [_Z17pg_process_bufferPiS_S_PfS0_fffiS_j_param_7];
	ld.param.u64 	%rd11, [_Z17pg_process_bufferPiS_S_PfS0_fffiS_j_param_9];
	ld.param.u32 	%r10, [_Z17pg_process_bufferPiS_S_PfS0_fffiS_j_param_10];
	mov.u32 	%r1, %ctaid.x;
	setp.ge.u32	%p1, %r1, %r10;
	@%p1 bra 	BB6_15;

	cvta.to.global.u64 	%rd12, %rd6;
	cvta.to.global.u64 	%rd13, %rd11;
	mul.wide.s32 	%rd14, %r1, 4;
	add.s64 	%rd15, %rd13, %rd14;
	ld.global.s32 	%rd1, [%rd15];
	shl.b64 	%rd16, %rd1, 2;
	add.s64 	%rd17, %rd12, %rd16;
	mov.u32 	%r2, %tid.x;
	ld.global.u32 	%r11, [%rd17];
	add.s32 	%r95, %r2, %r11;
	ld.global.u32 	%r4, [%rd17+4];
	setp.lt.s32	%p2, %r95, %r4;
	@%p2 bra 	BB6_3;

	mov.f64 	%fd28, 0d0000000000000000;
	bra.uni 	BB6_6;

BB6_3:
	cvta.to.global.u64 	%rd2, %rd8;
	cvta.to.global.u64 	%rd3, %rd9;
	cvta.to.global.u64 	%rd4, %rd7;
	mov.u32 	%r5, %ntid.x;
	mov.f32 	%f13, 0f00000000;

BB6_4:
	mul.wide.s32 	%rd18, %r95, 4;
	add.s64 	%rd19, %rd4, %rd18;
	ld.global.s32 	%rd20, [%rd19];
	shl.b64 	%rd21, %rd20, 2;
	add.s64 	%rd22, %rd3, %rd21;
	add.s64 	%rd23, %rd2, %rd21;
	ld.global.u32 	%r12, [%rd23];
	cvt.rn.f32.s32	%f7, %r12;
	ld.global.f32 	%f8, [%rd22];
	div.rn.f32 	%f9, %f8, %f7;
	fma.rn.f32 	%f13, %f9, %f5, %f13;
	add.s32 	%r95, %r5, %r95;
	setp.lt.s32	%p3, %r95, %r4;
	@%p3 bra 	BB6_4;

	cvt.f64.f32	%fd28, %f13;

BB6_6:
	cvta.to.global.u64 	%rd5, %rd10;
	bar.sync 	0;
	mov.b64 	 %rd24, %fd28;
	cvt.u32.u64	%r14, %rd24;
	mov.u32 	%r19, 16;
	mov.u32 	%r52, 31;
	// inline asm
	shfl.down.b32 %r13, %r14, %r19, %r52;
	// inline asm
	cvt.u64.u32	%rd25, %r13;
	shr.u64 	%rd26, %rd24, 32;
	cvt.u32.u64	%r18, %rd26;
	// inline asm
	shfl.down.b32 %r17, %r18, %r19, %r52;
	// inline asm
	cvt.u64.u32	%rd27, %r17;
	shl.b64 	%rd28, %rd27, 32;
	or.b64  	%rd29, %rd28, %rd25;
	mov.b64 	 %fd9, %rd29;
	add.f64 	%fd10, %fd28, %fd9;
	mov.b64 	 %rd30, %fd10;
	cvt.u32.u64	%r22, %rd30;
	mov.u32 	%r27, 8;
	// inline asm
	shfl.down.b32 %r21, %r22, %r27, %r52;
	// inline asm
	cvt.u64.u32	%rd31, %r21;
	shr.u64 	%rd32, %rd30, 32;
	cvt.u32.u64	%r26, %rd32;
	// inline asm
	shfl.down.b32 %r25, %r26, %r27, %r52;
	// inline asm
	cvt.u64.u32	%rd33, %r25;
	shl.b64 	%rd34, %rd33, 32;
	or.b64  	%rd35, %rd34, %rd31;
	mov.b64 	 %fd11, %rd35;
	add.f64 	%fd12, %fd10, %fd11;
	mov.b64 	 %rd36, %fd12;
	cvt.u32.u64	%r30, %rd36;
	mov.u32 	%r35, 4;
	// inline asm
	shfl.down.b32 %r29, %r30, %r35, %r52;
	// inline asm
	cvt.u64.u32	%rd37, %r29;
	shr.u64 	%rd38, %rd36, 32;
	cvt.u32.u64	%r34, %rd38;
	// inline asm
	shfl.down.b32 %r33, %r34, %r35, %r52;
	// inline asm
	cvt.u64.u32	%rd39, %r33;
	shl.b64 	%rd40, %rd39, 32;
	or.b64  	%rd41, %rd40, %rd37;
	mov.b64 	 %fd13, %rd41;
	add.f64 	%fd14, %fd12, %fd13;
	mov.b64 	 %rd42, %fd14;
	cvt.u32.u64	%r38, %rd42;
	mov.u32 	%r43, 2;
	// inline asm
	shfl.down.b32 %r37, %r38, %r43, %r52;
	// inline asm
	cvt.u64.u32	%rd43, %r37;
	shr.u64 	%rd44, %rd42, 32;
	cvt.u32.u64	%r42, %rd44;
	// inline asm
	shfl.down.b32 %r41, %r42, %r43, %r52;
	// inline asm
	cvt.u64.u32	%rd45, %r41;
	shl.b64 	%rd46, %rd45, 32;
	or.b64  	%rd47, %rd46, %rd43;
	mov.b64 	 %fd15, %rd47;
	add.f64 	%fd16, %fd14, %fd15;
	mov.b64 	 %rd48, %fd16;
	cvt.u32.u64	%r46, %rd48;
	mov.u32 	%r51, 1;
	// inline asm
	shfl.down.b32 %r45, %r46, %r51, %r52;
	// inline asm
	cvt.u64.u32	%rd49, %r45;
	shr.u64 	%rd50, %rd48, 32;
	cvt.u32.u64	%r50, %rd50;
	// inline asm
	shfl.down.b32 %r49, %r50, %r51, %r52;
	// inline asm
	cvt.u64.u32	%rd51, %r49;
	shl.b64 	%rd52, %rd51, 32;
	or.b64  	%rd53, %rd52, %rd49;
	mov.b64 	 %fd17, %rd53;
	add.f64 	%fd3, %fd16, %fd17;
	shr.u32 	%r8, %r2, 5;
	and.b32  	%r9, %r2, 31;
	setp.ne.s32	%p4, %r9, 0;
	@%p4 bra 	BB6_8;

	mul.wide.u32 	%rd54, %r8, 8;
	mov.u64 	%rd55, _Z16block_reduce_sumd$__cuda_local_var_35746_40_non_const_shared;
	add.s64 	%rd56, %rd55, %rd54;
	st.shared.f64 	[%rd56], %fd3;

BB6_8:
	bar.sync 	0;
	mov.u32 	%r53, %ntid.x;
	shr.u32 	%r54, %r53, 5;
	setp.lt.u32	%p5, %r2, %r54;
	@%p5 bra 	BB6_10;

	mov.f64 	%fd29, 0d0000000000000000;
	bra.uni 	BB6_11;

BB6_10:
	mul.wide.u32 	%rd57, %r9, 8;
	mov.u64 	%rd58, _Z16block_reduce_sumd$__cuda_local_var_35746_40_non_const_shared;
	add.s64 	%rd59, %rd58, %rd57;
	ld.shared.f64 	%fd29, [%rd59];

BB6_11:
	setp.ne.s32	%p6, %r8, 0;
	@%p6 bra 	BB6_13;

	mov.b64 	 %rd60, %fd29;
	cvt.u32.u64	%r56, %rd60;
	// inline asm
	shfl.down.b32 %r55, %r56, %r19, %r52;
	// inline asm
	cvt.u64.u32	%rd61, %r55;
	shr.u64 	%rd62, %rd60, 32;
	cvt.u32.u64	%r60, %rd62;
	// inline asm
	shfl.down.b32 %r59, %r60, %r19, %r52;
	// inline asm
	cvt.u64.u32	%rd63, %r59;
	shl.b64 	%rd64, %rd63, 32;
	or.b64  	%rd65, %rd64, %rd61;
	mov.b64 	 %fd19, %rd65;
	add.f64 	%fd20, %fd29, %fd19;
	mov.b64 	 %rd66, %fd20;
	cvt.u32.u64	%r64, %rd66;
	// inline asm
	shfl.down.b32 %r63, %r64, %r27, %r52;
	// inline asm
	cvt.u64.u32	%rd67, %r63;
	shr.u64 	%rd68, %rd66, 32;
	cvt.u32.u64	%r68, %rd68;
	// inline asm
	shfl.down.b32 %r67, %r68, %r27, %r52;
	// inline asm
	cvt.u64.u32	%rd69, %r67;
	shl.b64 	%rd70, %rd69, 32;
	or.b64  	%rd71, %rd70, %rd67;
	mov.b64 	 %fd21, %rd71;
	add.f64 	%fd22, %fd20, %fd21;
	mov.b64 	 %rd72, %fd22;
	cvt.u32.u64	%r72, %rd72;
	// inline asm
	shfl.down.b32 %r71, %r72, %r35, %r52;
	// inline asm
	cvt.u64.u32	%rd73, %r71;
	shr.u64 	%rd74, %rd72, 32;
	cvt.u32.u64	%r76, %rd74;
	// inline asm
	shfl.down.b32 %r75, %r76, %r35, %r52;
	// inline asm
	cvt.u64.u32	%rd75, %r75;
	shl.b64 	%rd76, %rd75, 32;
	or.b64  	%rd77, %rd76, %rd73;
	mov.b64 	 %fd23, %rd77;
	add.f64 	%fd24, %fd22, %fd23;
	mov.b64 	 %rd78, %fd24;
	cvt.u32.u64	%r80, %rd78;
	// inline asm
	shfl.down.b32 %r79, %r80, %r43, %r52;
	// inline asm
	cvt.u64.u32	%rd79, %r79;
	shr.u64 	%rd80, %rd78, 32;
	cvt.u32.u64	%r84, %rd80;
	// inline asm
	shfl.down.b32 %r83, %r84, %r43, %r52;
	// inline asm
	cvt.u64.u32	%rd81, %r83;
	shl.b64 	%rd82, %rd81, 32;
	or.b64  	%rd83, %rd82, %rd79;
	mov.b64 	 %fd25, %rd83;
	add.f64 	%fd26, %fd24, %fd25;
	mov.b64 	 %rd84, %fd26;
	cvt.u32.u64	%r88, %rd84;
	// inline asm
	shfl.down.b32 %r87, %r88, %r51, %r52;
	// inline asm
	cvt.u64.u32	%rd85, %r87;
	shr.u64 	%rd86, %rd84, 32;
	cvt.u32.u64	%r92, %rd86;
	// inline asm
	shfl.down.b32 %r91, %r92, %r51, %r52;
	// inline asm
	cvt.u64.u32	%rd87, %r91;
	shl.b64 	%rd88, %rd87, 32;
	or.b64  	%rd89, %rd88, %rd85;
	mov.b64 	 %fd27, %rd89;
	add.f64 	%fd29, %fd26, %fd27;

BB6_13:
	setp.ne.s32	%p7, %r2, 0;
	@%p7 bra 	BB6_15;

	cvt.rn.f32.f64	%f10, %fd29;
	add.f32 	%f11, %f10, %f3;
	add.f32 	%f12, %f11, %f4;
	add.s64 	%rd91, %rd5, %rd16;
	st.global.f32 	[%rd91], %f12;

BB6_15:
	ret;
}


